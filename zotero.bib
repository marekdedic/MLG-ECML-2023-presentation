
@inproceedings{kipf_semi-supervised_2017,
	address = {Toulon, France},
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	url = {https://openreview.net/forum?id=SJU4ayYgl},
	abstract = {Semi-supervised classification with a CNN model for graphs. State-of-the-art results on a number of citation network datasets.},
	language = {en},
	booktitle = {5th {International} {Conference} on {Learning} {Representations}, \{{ICLR}\} 2017, {Toulon}, {France}, {April} 24-26, 2017, {Conference} {Track} {Proceedings}},
	publisher = {OpenReview.net},
	author = {Kipf, Thomas N. and Welling, Max},
	month = apr,
	year = {2017},
}

@inproceedings{grover_node2vec_2016,
	title = {node2vec: {Scalable} feature learning for networks},
	shorttitle = {node2vec},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	author = {Grover, Aditya and Leskovec, Jure},
	year = {2016},
	pages = {855--864},
	file = {Full Text:/home/cisco/Zotero/storage/Y8U8YZQI/PMC5108654.html:text/html;Snapshot:/home/cisco/Zotero/storage/ITX5DJ45/2939672.html:text/html},
}

@inproceedings{perozzi_deepwalk_2014,
	title = {Deepwalk: {Online} learning of social representations},
	shorttitle = {Deepwalk},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
	year = {2014},
	pages = {701--710},
	file = {Full Text:/home/cisco/Zotero/storage/ISN68YG2/Perozzi et al. - 2014 - Deepwalk Online learning of social representation.pdf:application/pdf;Snapshot:/home/cisco/Zotero/storage/Y3TWLT7L/2623330.html:text/html},
}

@misc{kingma_adam:_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2020-06-25},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980},
	keywords = {Computer Science - Learning, Computer Science - Machine Learning},
}

@article{chen_harp_2018,
	title = {{HARP}: {Hierarchical} {Representation} {Learning} for {Networks}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	shorttitle = {{HARP}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11849},
	language = {en},
	number = {1},
	urldate = {2021-05-25},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Haochen and Perozzi, Bryan and Hu, Yifan and Skiena, Steven},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {network classification},
}

@inproceedings{schulz_mining_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Mining {Tree} {Patterns} with {Partially} {Injective} {Homomorphisms}},
	isbn = {978-3-030-10928-8},
	doi = {10.1007/978-3-030-10928-8_35},
	abstract = {One of the main differences between inductive logic programming (ILP) and graph mining lies in the pattern matching operator applied: While it is mainly defined by relational homomorphism (i.e., subsumption) in ILP, subgraph isomorphism is the most common pattern matching operator in graph mining. Using the fact that subgraph isomorphisms are injective homomorphisms, we bridge the gap between ILP and graph mining by considering a natural transition from homomorphisms to subgraph isomorphisms that is defined by partially injective homomorphisms, i.e., which require injectivity only for subsets of the vertex pairs in the pattern. Utilizing positive complexity results on deciding homomorphisms from bounded tree-width graphs, we present an algorithm mining frequent trees from arbitrary graphs w.r.t. partially injective homomorphisms. Our experimental results show that the predictive performance of the patterns obtained is comparable to that of ordinary frequent subgraphs. Thus, by preserving much from the advantageous properties of homomorphisms and subgraph isomorphisms, our approach provides a trade-off between efficiency and predictive power.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer International Publishing},
	author = {Schulz, Till Hendrik and Horv{\'a}th, Tam{\'a}s and Welke, Pascal and Wrobel, Stefan},
	editor = {Berlingerio, Michele and Bonchi, Francesco and G{\"a}rtner, Thomas and Hurley, Neil and Ifrim, Georgiana},
	year = {2019},
	pages = {585--601},
}

@inproceedings{tang_line_2015,
	title = {Line: {Large}-scale information network embedding},
	shorttitle = {Line},
	booktitle = {Proceedings of the 24th international conference on world wide web},
	author = {Tang, Jian and Qu, Meng and Wang, Mingzhe and Zhang, Ming and Yan, Jun and Mei, Qiaozhu},
	year = {2015},
	pages = {1067--1077},
}

@misc{huang_combining_2020,
	title = {Combining {Label} {Propagation} and {Simple} {Models} {Out}-performs {Graph} {Neural} {Networks}},
	abstract = {Graph Neural Networks (GNNs) are the predominant technique for learning over graphs. However, there is relatively little understanding of why GNNs are successful in practice and whether they are necessary for good performance. Here, we show that for many standard transductive node classification benchmarks, we can exceed or match the performance of state-of-the-art GNNs by combining shallow models that ignore the graph structure with two simple post-processing steps that exploit correlation in the label structure: (i) an "error correlation" that spreads residual errors in training data to correct errors in test data and (ii) a "prediction correlation" that smooths the predictions on the test data. We call this overall procedure Correct and Smooth (C\&S), and the post-processing steps are implemented via simple modifications to standard label propagation techniques from early graph-based semi-supervised learning methods. Our approach exceeds or nearly matches the performance of state-of-the-art GNNs on a wide variety of benchmarks, with just a small fraction of the parameters and orders of magnitude faster runtime. For instance, we exceed the best known GNN performance on the OGB-Products dataset with 137 times fewer parameters and greater than 100 times less training time. The performance of our methods highlights how directly incorporating label information into the learning algorithm (as was done in traditional techniques) yields easy and substantial performance gains. We can also incorporate our techniques into big GNN models, providing modest gains. Our code for the OGB results is at https://github.com/Chillee/CorrectAndSmooth.},
	urldate = {2021-06-04},
	publisher = {arXiv},
	author = {Huang, Qian and He, Horace and Singh, Abhay and Lim, Ser-Nam and Benson, Austin R.},
	month = nov,
	year = {2020},
	note = {arXiv:2010.13993},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks},
}

@article{salha_keep_2019,
	title = {Keep {It} {Simple}: {Graph} {Autoencoders} {Without} {Graph} {Convolutional} {Networks}},
	shorttitle = {Keep {It} {Simple}},
	url = {http://arxiv.org/abs/1910.00942},
	abstract = {Graph autoencoders (AE) and variational autoencoders (VAE) recently emerged as powerful node embedding methods, with promising performances on challenging tasks such as link prediction and node clustering. Graph AE, VAE and most of their extensions rely on graph convolutional networks (GCN) to learn vector space representations of nodes. In this paper, we propose to replace the GCN encoder by a simple linear model w.r.t. the adjacency matrix of the graph. For the two aforementioned tasks, we empirically show that this approach consistently reaches competitive performances w.r.t. GCN-based models for numerous real-world graphs, including the widely used Cora, Citeseer and Pubmed citation networks that became the de facto benchmark datasets for evaluating graph AE and VAE. This result questions the relevance of repeatedly using these three datasets to compare complex graph AE and VAE models. It also emphasizes the effectiveness of simple node encoding schemes for many real-world applications.},
	urldate = {2021-06-04},
	journal = {arXiv:1910.00942 [cs, stat]},
	author = {Salha, Guillaume and Hennequin, Romain and Vazirgiannis, Michalis},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.00942},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
}

@misc{frasca_sign_2020,
	title = {{SIGN}: {Scalable} {Inception} {Graph} {Neural} {Networks}},
	shorttitle = {{SIGN}},
	abstract = {Graph representation learning has recently been applied to a broad spectrum of problems ranging from computer graphics and chemistry to high energy physics and social media. The popularity of graph neural networks has sparked interest, both in academia and in industry, in developing methods that scale to very large graphs such as Facebook or Twitter social networks. In most of these approaches, the computational cost is alleviated by a sampling strategy retaining a subset of node neighbors or subgraphs at training time. In this paper we propose a new, efficient and scalable graph deep learning architecture which sidesteps the need for graph sampling by using graph convolutional filters of different size that are amenable to efficient precomputation, allowing extremely fast training and inference. Our architecture allows using different local graph operators (e.g. motif-induced adjacency matrices or Personalized Page Rank diffusion matrix) to best suit the task at hand. We conduct extensive experimental evaluation on various open benchmarks and show that our approach is competitive with other state-of-the-art architectures, while requiring a fraction of the training and inference time. Moreover, we obtain state-of-the-art results on ogbn-papers100M, the largest public graph dataset, with over 110 million nodes and 1.5 billion edges.},
	urldate = {2021-06-04},
	publisher = {arXiv},
	author = {Frasca, Fabrizio and Rossi, Emanuele and Eynard, Davide and Chamberlain, Ben and Bronstein, Michael and Monti, Federico},
	month = nov,
	year = {2020},
	note = {arXiv: 2004.11198},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
}

@inproceedings{paszke_pytorch_2019,
	address = {Vancouver, Canada},
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alch{\'e}-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {8024--8035},
}

@misc{fey_fast_2019,
	title = {Fast {Graph} {Representation} {Learning} with {PyTorch} {Geometric}},
	abstract = {We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios.},
	urldate = {2021-06-22},
	publisher = {arXiv},
	author = {Fey, Matthias and Lenssen, Jan Eric},
	month = apr,
	year = {2019},
	note = {arXiv: 1903.02428},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
}

@incollection{tang_pte_2015,
	address = {New York, NY, USA},
	title = {{PTE}: {Predictive} {Text} {Embedding} through {Large}-scale {Heterogeneous} {Text} {Networks}},
	isbn = {978-1-4503-3664-2},
	shorttitle = {{PTE}},
	url = {https://doi.org/10.1145/2783258.2783307},
	abstract = {Unsupervised text embedding methods, such as Skip-gram and Paragraph Vector, have been attracting increasing attention due to their simplicity, scalability, and effectiveness. However, comparing to sophisticated deep learning architectures such as convolutional neural networks, these methods usually yield inferior results when applied to particular machine learning tasks. One possible reason is that these text embedding methods learn the representation of text in a fully unsupervised way, without leveraging the labeled information available for the task. Although the low dimensional representations learned are applicable to many different tasks, they are not particularly tuned for any task. In this paper, we fill this gap by proposing a semi-supervised representation learning method for text data, which we call the predictive text embedding (PTE). Predictive text embedding utilizes both labeled and unlabeled data to learn the embedding of text. The labeled information and different levels of word co-occurrence information are first represented as a large-scale heterogeneous text network, which is then embedded into a low dimensional space through a principled and efficient algorithm. This low dimensional embedding not only preserves the semantic closeness of words and documents, but also has a strong predictive power for the particular task. Compared to recent supervised approaches based on convolutional neural networks, predictive text embedding is comparable or more effective, much more efficient, and has fewer parameters to tune.},
	urldate = {2021-06-30},
	booktitle = {Proceedings of the 21th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Tang, Jian and Qu, Meng and Mei, Qiaozhu},
	month = aug,
	year = {2015},
	keywords = {predictive text embedding, representation learning},
	pages = {1165--1174},
}

@inproceedings{topping_understanding_2021,
	title = {Understanding over-squashing and bottlenecks on graphs via curvature},
	url = {https://openreview.net/forum?id=7UmjRGzp-A},
	abstract = {Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from...},
	language = {en},
	urldate = {2022-06-12},
	booktitle = {The {Tenth} {International} {Conference} on {Learning} {Representations}},
	author = {Topping, Jake and Giovanni, Francesco Di and Chamberlain, Benjamin Paul and Dong, Xiaowen and Bronstein, Michael M.},
	month = sep,
	year = {2021},
}

@misc{velickovic_geometric_2021,
	title = {Geometric {Deep} {Learning} - {Grids}, {Groups}, {Graphs}, {Geodesics}, and {Gauges}},
	url = {https://geometricdeeplearning.com/},
	abstract = {Grids, Groups, Graphs, Geodesics, and Gauges},
	urldate = {2022-06-12},
	author = {Veli{\v c}kovi{\'c}, Petar},
	year = {2021},
}

@inproceedings{yang_revisiting_2016,
	address = {New York, NY, USA},
	title = {Revisiting {Semi}-{Supervised} {Learning} with {Graph} {Embeddings}},
	url = {https://proceedings.mlr.press/v48/yanga16.html},
	abstract = {We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models.},
	language = {en},
	urldate = {2022-06-16},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yang, Zhilin and Cohen, William and Salakhudinov, Ruslan},
	month = jun,
	year = {2016},
	pages = {40--48},
}

@inproceedings{chen_fastgcn_2018,
	title = {{FastGCN}: {Fast} {Learning} with {Graph} {Convolutional} {Networks} via {Importance} {Sampling}},
	shorttitle = {{FastGCN}},
	url = {https://openreview.net/forum?id=rytstxWAW},
	abstract = {The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. Such a model, however, is transductive in nature because...},
	language = {en},
	urldate = {2022-06-16},
	booktitle = {6th {International} {Conference} on {Learning} {Representations}},
	author = {Chen, Jie and Ma, Tengfei and Xiao, Cao},
	month = feb,
	year = {2018},
}

@inproceedings{bojchevski_deep_2018,
	title = {Deep {Gaussian} {Embedding} of {Graphs}: {Unsupervised} {Inductive} {Learning} via {Ranking}},
	shorttitle = {Deep {Gaussian} {Embedding} of {Graphs}},
	url = {https://openreview.net/forum?id=r1ZdKJ-0W},
	abstract = {We embed nodes in a graph as Gaussian distributions allowing us to capture uncertainty about their representation.},
	language = {en},
	urldate = {2022-06-16},
	booktitle = {6th {International} {Conference} on {Learning} {Representations}},
	author = {Bojchevski, Aleksandar and G{\"u}nnemann, Stephan},
	month = feb,
	year = {2018},
}

@article{srivastava_dropout_2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	volume = {15},
	issn = {1533-7928},
	shorttitle = {Dropout},
	url = {http://jmlr.org/papers/v15/srivastava14a.html},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different {\^a}??thinned{\^a}?? networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	number = {56},
	urldate = {2022-06-16},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958},
}

@inproceedings{defferrard_convolutional_2016,
	address = {Barcelona, Spain},
	title = {Convolutional {Neural} {Networks} on {Graphs} with {Fast} {Localized} {Spectral} {Filtering}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/file/04df4d434d481c5bb723be1b6df1ee65-Paper.pdf},
	abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words{\textquoteright} embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
	urldate = {2022-06-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Defferrard, Micha{\"e}l and Bresson, Xavier and Vandergheynst, Pierre},
	year = {2016},
}

@inproceedings{chamberlain_grand_2021,
	title = {{GRAND}: {Graph} {Neural} {Diffusion}},
	shorttitle = {{GRAND}},
	url = {https://openreview.net/forum?id=_1fu_cjsaRE},
	abstract = {We present Graph Neural Diffusion (GRAND), a model that approaches deep learning on graphs as a continuous diffusion process and treats Graph Neural Networks (GNNs) as discretisations of an...},
	language = {en},
	urldate = {2022-06-17},
	booktitle = {The {Symbiosis} of {Deep} {Learning} and {Differential} {Equations}},
	author = {Chamberlain, Benjamin Paul and Rowbottom, James and Gorinova, Maria I. and Webb, Stefan D. and Rossi, Emanuele and Bronstein, Michael M.},
	month = sep,
	year = {2021},
}

@article{zhang_eigen-gnn_2021,
	title = {Eigen-{GNN}: a {Graph} {Structure} {Preserving} {Plug}-in for {GNNs}},
	issn = {1558-2191},
	shorttitle = {Eigen-{GNN}},
	doi = {10.1109/TKDE.2021.3112746},
	abstract = {Graph Neural Networks (GNNs) are emerging machine learning models on graphs. Although sufficiently deep GNNs are shown theoretically capable of fully preserving graph structures, most existing GNN models in practice are shallow and essentially feature-centric. We show empirically and analytically that the existing shallow GNNs cannot preserve graph structures well. To overcome this fundamental challenge, we propose Eigen-GNN, a simple yet effective and general plug-in module to boost GNNs ability in preserving graph structures. Specifically, we integrate the eigenspace of graph structures with GNNs by treating GNNs as a type of dimensionality reduction and expanding the initial dimensionality reduction bases. Without needing to increase depths, Eigen-GNN possesses more flexibilities in handling both feature-driven and structure-driven tasks since the initial bases contain both node features and graph structures. We present extensive experimental results to demonstrate the effectiveness of Eigen-GNN for tasks including node classification, link prediction, and graph isomorphism tests.},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhang, Ziwei and Cui, Peng and Pei, Jian and Wang, Xin and Zhu, Wenwu},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Convolution, Convolutional codes, Dimensionality Reduction, Eigenvector, Graph Neural Networks, Graph Structure, Laplace equations, Smoothing methods, Social networking (online), Task analysis, Training},
	pages = {1--1},
}

@inproceedings{gasteiger_diffusion_2019,
	address = {Vancouver, Canada},
	title = {Diffusion {Improves} {Graph} {Learning}},
	volume = {32},
	url = {https://proceedings.neurips.cc/file/2019/hash/23c894276a2c5a16470e6a31f4618d73-Paper.pdf},
	abstract = {Graph convolution is the core of most Graph Neural Networks (GNNs) and usually approximated by message passing between direct (one-hop) neighbors. In this work, we remove the restriction of using only the direct neighbors by introducing a powerful, yet spatially localized graph convolution: Graph diffusion convolution (GDC). GDC leverages generalized graph diffusion, examples of which are the heat kernel and personalized PageRank. It alleviates the problem of noisy and often arbitrarily defined edges in real graphs. We show that GDC is closely related to spectral-based models and thus combines the strengths of both spatial (message passing) and spectral methods. We demonstrate that replacing message passing with graph diffusion convolution consistently leads to significant performance improvements across a wide range of models on both supervised and unsupervised tasks and a variety of datasets. Furthermore, GDC is not limited to GNNs but can trivially be combined with any graph-based model or algorithm (e.g. spectral clustering) without requiring any changes to the latter or affecting its computational complexity. Our implementation is available online.},
	urldate = {2022-06-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Gasteiger, Johannes and Wei{\ss} enberger, Stefan and G{\"u}nnemann, Stephan},
	year = {2019},
}

@misc{page_pagerank_1999,
	type = {Techreport},
	title = {The {PageRank} {Citation} {Ranking}: {Bringing} {Order} to the {Web}.},
	shorttitle = {The {PageRank} {Citation} {Ranking}},
	url = {http://ilpubs.stanford.edu:8090/422/},
	abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.},
	urldate = {2022-06-18},
	author = {Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry},
	month = nov,
	year = {1999},
	note = {Publisher: Stanford InfoLab},
}

@inproceedings{kondor_diffusion_2002,
	title = {Diffusion kernels on graphs and other discrete structures},
	volume = {2002},
	booktitle = {Proceedings of the 19th international conference on machine learning},
	author = {Kondor, Risi Imre and Lafferty, John},
	year = {2002},
	pages = {315--322},
}

@inproceedings{morris_tudataset_2020,
	title = {{TUDataset}: {A} collection of benchmark datasets for learning with graphs},
	url = {www.graphlearning.io},
	booktitle = {{ICML} 2020 {Workshop} on {Graph} {Representation} {Learning} and {Beyond} ({GRL}+ 2020)},
	author = {Morris, Christopher and Kriege, Nils M. and Bause, Franka and Kersting, Kristian and Mutzel, Petra and Neumann, Marion},
	year = {2020},
}

@techreport{dedic_graph_2021,
	title = {Graph {Coarsening} {Can} {Increase} {Learning} {Efficiency}},
	copyright = {All rights reserved},
	url = {http://kmwww.fjfi.cvut.cz/ddny},
	abstract = {Graph based models are used for tasks with increasing size and computational demands. The paper
focuses on leveraging methods for pretraining on coarser graphs with HARP as the method of choice. The
method is generalized using partially injective homomorphisms, a concept from the field of data mining.
Such a way of producing graph coarsenings is shown to be feasible and not to affect the performance of
HARP in a negative way. Also, the performance-complexity characterics of these methods are studied
and HARP is established as a way of efficient pretraining which can reduce the ammount of computational
power needed to train graph-based models on large data.},
	institution = {Czech Technical University in Prague},
	author = {D{\v e}di{\v c}, Marek},
	month = nov,
	year = {2021},
	pages = {13--21},
}

@inproceedings{akyildiz_understanding_2020,
	title = {Understanding {Coarsening} for {Embedding} {Large}-{Scale} {Graphs}},
	doi = {10.1109/BigData50022.2020.9377898},
	abstract = {A significant portion of the data today, e.g, social networks, web connections, etc., can be modeled by graphs. A proper analysis of graphs with Machine Learning (ML) algorithms has the potential to yield far-reaching insights into many areas of research and industry. However, the irregular structure of graph data constitutes an obstacle for running ML tasks on graphs such as link prediction, node classification, and anomaly detection. Graph embedding is a compute-intensive process of representing graphs as a set of vectors in a d-dimensional space, which in turn makes it amenable to ML tasks. Many approaches have been proposed in the literature to improve the performance of graph embedding, e.g., using distributed algorithms, accelerators, and pre-processing techniques. Graph coarsening, which can be considered a pre-processing step, is a structural approximation of a given, large graph with a smaller one. As the literature suggests, the cost of embedding significantly decreases when coarsening is employed. In this work, we thoroughly analyze the impact of the coarsening quality on the embedding performance both in terms of speed and accuracy. Our experiments with a state-of-the-art, fast graph embedding tool show that there is an interplay between the coarsening decisions taken and the embedding quality.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Akyildiz, Taha Atahan and Alabsi Aljundi, Amro and Kaya, Kamer},
	month = dec,
	year = {2020},
	keywords = {Machine learning, Machine learning algorithms, Social networking (online), Task analysis, Big Data, Graph coarsening, graph embedding, multi-level approach, Prediction algorithms, Tools},
	pages = {2937--2946},
}

@inproceedings{huang_scaling_2021,
	address = {New York, NY, USA},
	series = {{KDD} '21},
	title = {Scaling {Up} {Graph} {Neural} {Networks} {Via} {Graph} {Coarsening}},
	isbn = {978-1-4503-8332-5},
	doi = {10.1145/3447548.3467256},
	abstract = {Scalability of graph neural networks remains one of the major challenges in graph machine learning. Since the representation of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes from previous layers, the receptive fields grow exponentially, which makes standard stochastic optimization techniques ineffective. Various approaches have been proposed to alleviate this issue, e.g., sampling-based methods and techniques based on pre-computation of graph filters. In this paper, we take a different approach and propose to use graph coarsening for scalable training of GNNs, which is generic, extremely simple and has sublinear memory and time costs during training. We present extensive theoretical analysis on the effect of using coarsening operations and provides useful guidance on the choice of coarsening methods. Interestingly, our theoretical analysis shows that coarsening can also be considered as a type of regularization and may improve the generalization. Finally, empirical results on real world datasets show that, simply applying off-the-shelf coarsening methods, we can reduce the number of nodes by up to a factor of ten without causing a noticeable downgrade in classification accuracy.},
	urldate = {2022-06-20},
	booktitle = {Proceedings of the 27th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Zengfeng and Zhang, Shengzhong and Xi, Chong and Liu, Tang and Zhou, Min},
	month = aug,
	year = {2021},
	keywords = {graph coarsening, graph neural networks, scalable training},
	pages = {675--684},
}

@article{loukas_graph_2019,
	title = {Graph {Reduction} with {Spectral} and {Cut} {Guarantees}.},
	volume = {20},
	number = {116},
	journal = {J. Mach. Learn. Res.},
	author = {Loukas, Andreas},
	year = {2019},
	pages = {1--42},
}

@article{makarov_survey_2021,
	title = {Survey on graph embeddings and their applications to machine learning problems on graphs},
	volume = {7},
	issn = {2376-5992},
	url = {https://peerj.com/articles/cs-357},
	doi = {10.7717/peerj-cs.357},
	abstract = {Dealing with relational data always required significant computational resources, domain expertise and task-dependent feature engineering to incorporate structural information into a predictive model. Nowadays, a family of automated graph feature engineering techniques has been proposed in different streams of literature. So-called graph embeddings provide a powerful tool to construct vectorized feature spaces for graphs and their components, such as nodes, edges and subgraphs under preserving inner graph properties. Using the constructed feature spaces, many machine learning problems on graphs can be solved via standard frameworks suitable for vectorized feature representation. Our survey aims to describe the core concepts of graph embeddings and provide several taxonomies for their description. First, we start with the methodological approach and extract three types of graph embedding models based on matrix factorization, random-walks and deep learning approaches. Next, we describe how different types of networks impact the ability of models to incorporate structural and attributed data into a unified embedding. Going further, we perform a thorough evaluation of graph embedding applications to machine learning problems on graphs, among which are node classification, link prediction, clustering, visualization, compression, and a family of the whole graph embedding algorithms suitable for graph classification, similarity and alignment problems. Finally, we overview the existing applications of graph embeddings to computer science domains, formulate open problems and provide experiment results, explaining how different networks properties result in graph embeddings quality in the four classic machine learning problems on graphs, such as node classification, link prediction, clustering and graph visualization. As a result, our survey covers a new rapidly growing field of network feature engineering, presents an in-depth analysis of models based on network types, and overviews a wide range of applications to machine learning problems on graphs.},
	language = {en},
	urldate = {2022-06-21},
	journal = {PeerJ Computer Science},
	author = {Makarov, Ilya and Kiselev, Dmitrii and Nikitinsky, Nikita and Subelj, Lovro},
	month = feb,
	year = {2021},
	note = {Publisher: PeerJ Inc.},
	pages = {e357},
}

@article{chen_graph_2022,
	title = {Graph coarsening: from scientific computing to machine learning},
	volume = {79},
	issn = {2281-7875},
	shorttitle = {Graph coarsening},
	doi = {10.1007/s40324-021-00282-x},
	abstract = {The general method of graph coarsening or graph reduction has been a remarkably useful and ubiquitous tool in scientific computing and it is now just starting to have a similar impact in machine learning. The goal of this paper is to take a broad look into coarsening techniques that have been successfully deployed in scientific computing and see how similar principles are finding their way in more recent applications related to machine learning. In scientific computing, coarsening plays a central role in algebraic multigrid methods as well as the related class of multilevel incomplete LU factorizations. In machine learning, graph coarsening goes under various names, e.g., graph downsampling or graph reduction. Its goal in most cases is to replace some original graph by one which has fewer nodes, but whose structure and characteristics are similar to those of the original graph. As will be seen, a common strategy in these methods is to rely on spectral properties to define the coarse graph.},
	language = {en},
	number = {1},
	urldate = {2022-06-22},
	journal = {SeMA Journal},
	author = {Chen, Jie and Saad, Yousef and Zhang, Zechen},
	month = mar,
	year = {2022},
	keywords = {05C85, 65F10, 65N55m, 68T05, 94C15, Coarsening, Graph Coarsening, Graphs and Networks, Hierarchical methods. Graph Neural Networks, Multilevel methods},
	pages = {187--223},
}

@article{xie_graph_2020,
	title = {Graph convolutional networks with multi-level coarsening for graph classification},
	volume = {194},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120300629},
	doi = {10.1016/j.knosys.2020.105578},
	abstract = {Graph convolutional networks (GCNs) have attracted increasing attention in recent years. Many important tasks in graph analysis involve graph classification which aims to map a graph to a certain category. However, as the number of convolutional layers increases, most existing GCNs suffer from the problem of over-smoothing, which makes it difficult to extract the hierarchical information and global patterns of graphs when learning its representations. In this paper, we propose a multi-level coarsening based GCN (MLC-GCN) for graph classification. Specifically, from the perspective of graph analysis, we develop new insights into the convolutional architecture of image classification. Inspired by this, the two-stage MLC-GCN architecture is presented. In the architecture, we first introduce an adaptive structural coarsening module to produce a series of coarsened graphs and then construct the convolutional network based on these graphs. In contrast to existing GCNs, MLC-GCN has the advantages of learning graph representations at multiple levels while preserving the local and global information of graphs. Experimental results on multiple benchmark datasets demonstrate that the proposed MLC-GCN method is competitive with the state-of-the-art graph classification methods.},
	language = {en},
	urldate = {2022-06-22},
	journal = {Knowledge-Based Systems},
	author = {Xie, Yu and Yao, Chuanyu and Gong, Maoguo and Chen, Cheng and Qin, A. K.},
	month = apr,
	year = {2020},
	keywords = {Graph classification, Graph convolutional networks, Multi-level coarsening},
	pages = {105578},
}

@article{zhang_harp_2021,
	title = {{HARP} {Pro}: {Hierarchical} {Representation} {Learning} based on global and local features for social networks},
	shorttitle = {{HARP} {Pro}},
	author = {Zhang, Wei and Yang, Jing and Shang, Fanshu},
	year = {2021},
}

@inproceedings{bravo_hermsdorff_unifying_2019,
	address = {Vancouver, Canada},
	title = {A {Unifying} {Framework} for {Spectrum}-{Preserving} {Graph} {Sparsification} and {Coarsening}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/file/cd474f6341aeffd65f93084d0dae3453-Paper.pdf},
	urldate = {2022-06-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bravo Hermsdorff, Gecia and Gunderson, Lee},
	year = {2019},
}

@misc{li_deepergcn_2020,
	title = {{DeeperGCN}: {All} {You} {Need} to {Train} {Deeper} {GCNs}},
	shorttitle = {{DeeperGCN}},
	url = {http://arxiv.org/abs/2006.07739},
	doi = {10.48550/arXiv.2006.07739},
	abstract = {Graph Convolutional Networks (GCNs) have been drawing significant attention with the power of representation learning on graphs. Unlike Convolutional Neural Networks (CNNs), which are able to take advantage of stacking very deep layers, GCNs suffer from vanishing gradient, over-smoothing and over-fitting issues when going deeper. These challenges limit the representation power of GCNs on large-scale graphs. This paper proposes DeeperGCN that is capable of successfully and reliably training very deep GCNs. We define differentiable generalized aggregation functions to unify different message aggregation operations (e.g. mean, max). We also propose a novel normalization layer namely MsgNorm and a pre-activation version of residual connections for GCNs. Extensive experiments on Open Graph Benchmark (OGB) show DeeperGCN significantly boosts performance over the state-of-the-art on the large scale graph learning tasks of node property prediction and graph property prediction. Please visit https://www.deepgcns.org for more information.},
	urldate = {2022-06-22},
	publisher = {arXiv},
	author = {Li, Guohao and Xiong, Chenxin and Thabet, Ali and Ghanem, Bernard},
	month = jun,
	year = {2020},
	note = {Number: arXiv:2006.07739
arXiv:2006.07739 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{deng_graphzoom_2020,
	title = {{GraphZoom}: {A} {Multi}-level {Spectral} {Approach} for {Accurate} and {Scalable} {Graph} {Embedding}},
	shorttitle = {{GraphZoom}},
	url = {https://openreview.net/forum?id=r1lGO0EKDH},
	abstract = {A multi-level spectral approach to improving the quality and scalability of unsupervised graph embedding.},
	language = {en},
	urldate = {2022-06-23},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Deng, Chenhui and Zhao, Zhiqiang and Wang, Yongyu and Zhang, Zhiru and Feng, Zhuo},
	month = mar,
	year = {2020},
}

@inproceedings{cai_graph_2022,
	title = {Graph {Coarsening} with {Neural} {Networks}},
	url = {https://openreview.net/forum?id=uxpzitPEooJ},
	abstract = {As large scale-graphs become increasingly more prevalent, it poses significant computational challenges to process, extract and analyze large graph data. Graph coarsening is one popular technique...},
	language = {en},
	urldate = {2022-06-23},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Cai, Chen and Wang, Dingkang and Wang, Yusu},
	month = feb,
	year = {2022},
}

@article{fortin_deap_2012,
	title = {{DEAP}: {Evolutionary} {Algorithms} {Made} {Easy}},
	volume = {13},
	journal = {Journal of Machine Learning Research},
	author = {Fortin, F{\'e}lix-Antoine and Rainville, Fran{\c c}ois-Michel De and Gardner, Marc-Andr{\'e} and Parizeau, Marc and Gagn{\'e}, Christian},
	month = jul,
	year = {2012},
	pages = {2171--2175},
}

@inproceedings{prochazka_downstream_2022,
	address = {Zuberec, Slovakia},
	title = {Downstream {Task} {Aware} {Scalable} {Graph} {Size} {Reduction} for {Efficient} {GNN} {Application} on {Big} {Data}},
	booktitle = {Information {Technologies} - {Applications} and {Theory} ({ITAT} 2022)},
	author = {Proch{\'a}zka, Pavel and Mare{\v s}, Michal and D{\v e}di{\v c}, Marek},
	year = {2022},
}

@misc{kammer_space-efficient_2022,
	title = {Space-{Efficient} {Graph} {Coarsening} with {Applications} to {Succinct} {Planar} {Encodings}},
	abstract = {We present a novel space-efficient graph coarsening technique for \$n\$-vertex planar graphs \$G\$, called {\textbackslash}textit\{cloud partition\}, which partitions the vertices \$V(G)\$ into disjoint sets \$C\$ of size \$O({\textbackslash}log n)\$ such that each \$C\$ induces a connected subgraph of \$G\$. Using this partition \${\textbackslash}mathcal\{P\}\$ we construct a so-called {\textbackslash}textit\{structure-maintaining minor\} \$F\$ of \$G\$ via specific contractions within the disjoint sets such that \$F\$ has \$O(n/{\textbackslash}log n)\$ vertices. The combination of \$(F, {\textbackslash}mathcal\{P\})\$ is referred to as a {\textbackslash}textit\{cloud decomposition\}. For planar graphs we show that a cloud decomposition can be constructed in \$O(n)\$ time and using \$O(n)\$ bits. Given a cloud decomposition \$(F, {\textbackslash}mathcal\{P\})\$ constructed for a planar graph \$G\$ we are able to find a balanced separator of \$G\$ in \$O(n/{\textbackslash}log n)\$ time. Contrary to related publications, we do not make use of an embedding of the planar input graph. We generalize our cloud decomposition from planar graphs to \$H\$-minor-free graphs for any fixed graph \$H\$. This allows us to construct the succinct encoding scheme for \$H\$-minor-free graphs due to Blelloch and Farzan (CPM 2010) in \$O(n)\$ time and \$O(n)\$ bits improving both runtime and space by a factor of \${\textbackslash}Theta({\textbackslash}log n)\$. As an additional application of our cloud decomposition we show that, for \$H\$-minor-free graphs, a tree decomposition of width \$O(n{\textasciicircum}\{1/2 + {\textbackslash}epsilon\})\$ for any \${\textbackslash}epsilon {\textgreater} 0\$ can be constructed in \$O(n)\$ bits and a time linear in the size of the tree decomposition. A similar result by Izumi and Otachi (ICALP 2020) constructs a tree decomposition of width \$O(k {\textbackslash}sqrt\{n\} {\textbackslash}log n)\$ for graphs of treewidth \$k {\textbackslash}leq {\textbackslash}sqrt\{n\}\$ in sublinear space and polynomial time.},
	urldate = {2022-08-10},
	publisher = {arXiv},
	author = {Kammer, Frank and Meintrup, Johannes},
	month = jun,
	year = {2022},
	note = {arXiv:2205.06128},
	keywords = {Computer Science - Data Structures and Algorithms},
}

@misc{liu_comprehensive_2022,
	title = {Comprehensive {Graph} {Gradual} {Pruning} for {Sparse} {Training} in {Graph} {Neural} {Networks}},
	abstract = {Graph Neural Networks (GNNs) tend to suffer from high computation costs due to the exponentially increasing scale of graph data and the number of model parameters, which restricts their utility in practical applications. To this end, some recent works focus on sparsifying GNNs with the lottery ticket hypothesis (LTH) to reduce inference costs while maintaining performance levels. However, the LTH-based methods suffer from two major drawbacks: 1) they require exhaustive and iterative training of dense models, resulting in an extremely large training computation cost, and 2) they only trim graph structures and model parameters but ignore the node feature dimension, where significant redundancy exists. To overcome the above limitations, we propose a comprehensive graph gradual pruning framework termed CGP. This is achieved by designing a during-training graph pruning paradigm to dynamically prune GNNs within one training process. Unlike LTH-based methods, the proposed CGP approach requires no re-training, which significantly reduces the computation costs. Furthermore, we design a co-sparsifying strategy to comprehensively trim all three core elements of GNNs: graph structures, node features, and model parameters. Meanwhile, aiming at refining the pruning operation, we introduce a regrowth process into our CGP framework, in order to re-establish the pruned but important connections. The proposed CGP is evaluated by using a node classification task across 6 GNN architectures, including shallow models (GCN and GAT), shallow-but-deep-propagation models (SGC and APPNP), and deep models (GCNII and ResGCN), on a total of 14 real-world graph datasets, including large-scale graph datasets from the challenging Open Graph Benchmark. Experiments reveal that our proposed strategy greatly improves both training and inference efficiency while matching or even exceeding the accuracy of existing methods.},
	language = {en},
	urldate = {2022-08-12},
	publisher = {arXiv},
	author = {Liu, Chuang and Ma, Xueqi and Zhan, Yibing and Ding, Liang and Tao, Dapeng and Du, Bo and Hu, Wenbin and Mandic, Danilo},
	month = jul,
	year = {2022},
	note = {arXiv:2207.08629},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Liu et al. - 2022 - Comprehensive Graph Gradual Pruning for Sparse Tra.pdf:/home/cisco/Zotero/storage/N7N2RGPM/Liu et al. - 2022 - Comprehensive Graph Gradual Pruning for Sparse Tra.pdf:application/pdf},
}

@inproceedings{chiang_cluster-gcn_2019,
	address = {New York, NY, USA},
	series = {{KDD} '19},
	title = {Cluster-{GCN}: {An} {Efficient} {Algorithm} for {Training} {Deep} and {Large} {Graph} {Convolutional} {Networks}},
	isbn = {978-1-4503-6201-6},
	shorttitle = {Cluster-{GCN}},
	url = {https://doi.org/10.1145/3292500.3330925},
	doi = {10.1145/3292500.3330925},
	abstract = {Graph convolutional network (GCN) has been successfully applied to many graph-based applications; however, training a large-scale GCN remains challenging. Current SGD-based algorithms suffer from either a high computational cost that exponentially grows with number of GCN layers, or a large space requirement for keeping the entire graph and the embedding of each node in memory. In this paper, we propose Cluster-GCN, a novel GCN algorithm that is suitable for SGD-based training by exploiting the graph clustering structure. Cluster-GCN works as the following: at each step, it samples a block of nodes that associate with a dense subgraph identified by a graph clustering algorithm, and restricts the neighborhood search within this subgraph. This simple but effective strategy leads to significantly improved memory and computational efficiency while being able to achieve comparable test accuracy with previous algorithms. To test the scalability of our algorithm, we create a new Amazon2M data with 2 million nodes and 61 million edges which is more than 5 times larger than the previous largest publicly available dataset (Reddit). For training a 3-layer GCN on this data, Cluster-GCN is faster than the previous state-of-the-art VR-GCN (1523 seconds vs 1961 seconds) and using much less memory (2.2GB vs 11.2GB). Furthermore, for training 4 layer GCN on this data, our algorithm can finish in around 36 minutes while all the existing GCN training algorithms fail to train due to the out-of-memory issue. Furthermore, Cluster-GCN allows us to train much deeper GCN without much time and memory overhead, which leads to improved prediction accuracy---using a 5-layer Cluster-GCN, we achieve state-of-the-art test F1 score 99.36 on the PPI dataset, while the previous best result was 98.71 by{\textasciitilde}{\textbackslash}citezhang2018gaan.},
	urldate = {2022-11-28},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Chiang, Wei-Lin and Liu, Xuanqing and Si, Si and Li, Yang and Bengio, Samy and Hsieh, Cho-Jui},
	month = jul,
	year = {2019},
	keywords = {clustering, deep learning, graph convolutional networks, large-scale learning, semi-supervised learning},
	pages = {257--266},
}

@article{rozemberczki_multi-scale_2021,
	title = {Multi-{Scale} attributed node embedding},
	volume = {9},
	issn = {2051-1329},
	url = {https://doi.org/10.1093/comnet/cnab014},
	doi = {10.1093/comnet/cnab014},
	abstract = {We present network embedding algorithms that capture information about a node from the local distribution over node attributes around it, as observed over random walks following an approach similar to Skip-gram. Observations from neighbourhoods of different sizes are either pooled (AE) or encoded distinctly in a multi-scale approach (MUSAE). Capturing attribute-neighbourhood relationships over multiple scales is useful for a range of applications, including latent feature identification across disconnected networks with similar features. We prove theoretically that matrices of node-feature pointwise mutual information are implicitly factorized by the embeddings. Experiments show that our algorithms are computationally efficient and outperform comparable models on social networks and web graphs.},
	number = {2},
	urldate = {2022-11-28},
	journal = {Journal of Complex Networks},
	author = {Rozemberczki, Benedek and Allen, Carl and Sarkar, Rik},
	month = apr,
	year = {2021},
	pages = {cnab014},
}

@article{herrmann_multilevel_2019,
	title = {Multilevel {Algorithms} for {Acyclic} {Partitioning} of {Directed} {Acyclic} {Graphs}},
	volume = {41},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/abs/10.1137/18M1176865},
	doi = {10.1137/18M1176865},
	abstract = {We investigate the problem of partitioning the vertices of a directed acyclic graph into a given number of parts. The objective function is to minimize the number or the total weight of the edges having end points in different parts, which is also known as the edge cut. The standard load balancing constraint of having an equitable partition of the vertices among the parts should be met. Furthermore, the partition is required to be acyclic; i.e., the interpart edges between the vertices from different parts should preserve an acyclic dependency structure among the parts. In this work, we adopt the multilevel approach with coarsening, initial partitioning, and refinement phases for acyclic partitioning of directed acyclic graphs. We focus on two-way partitioning (sometimes called bisection), as this scheme can be used in a recursive way for multiway partitioning. To ensure the acyclicity of the partition at all times, we propose novel and efficient coarsening and refinement heuristics. The quality of the computed acyclic partitions is assessed by computing the edge cut. We also propose effective ways to use the standard undirected graph partitioning methods in our multilevel scheme. We perform a large set of experiments on a dataset consisting of (i) graphs coming from an application and (ii) some others corresponding to matrices from a public collection. We report significant improvements compared to the current state of the art.},
	number = {4},
	urldate = {2022-11-28},
	journal = {SIAM Journal on Scientific Computing},
	author = {Herrmann, Julien and {\"O}zkaya, M. Yusuf and U{\c c}ar, Bora and Kaya, Kamer and {\c C}ataly{\"u}rek, {\"U}mit V.},
	month = jan,
	year = {2019},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {05C85, 05C70, 68R10, 68W05, acyclic partitioning, directed graph, multilevel partitioning},
	pages = {A2117--A2145},
}

@article{bethune_hierarchical_2020,
	title = {Hierarchical and {Unsupervised} {Graph} {Representation} {Learning} with {Loukas}{\textquoteright}s {Coarsening}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1999-4893},
	url = {https://www.mdpi.com/1999-4893/13/9/206},
	doi = {10.3390/a13090206},
	abstract = {We propose a novel algorithm for unsupervised graph representation learning with attributed graphs. It combines three advantages addressing some current limitations of the literature: (i) The model is inductive: it can embed new graphs without re-training in the presence of new data; (ii) The method takes into account both micro-structures and macro-structures by looking at the attributed graphs at different scales; (iii) The model is end-to-end differentiable: it is a building block that can be plugged into deep learning pipelines and allows for back-propagation. We show that combining a coarsening method having strong theoretical guarantees with mutual information maximization suffices to produce high quality embeddings. We evaluate them on classification tasks with common benchmarks of the literature. We show that our algorithm is competitive with state of the art among unsupervised graph representation learning methods.},
	language = {en},
	number = {9},
	urldate = {2022-11-28},
	journal = {Algorithms},
	author = {B{\'e}thune, Louis and Kaloga, Yacouba and Borgnat, Pierre and Garivier, Aur{\'e}lien and Habrard, Amaury},
	month = sep,
	year = {2020},
	note = {Number: 9
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {graph coarsening, graph convolutional networks, graph representation learning, Graph2Vec, mutual information maximization, unsupervised learning},
	pages = {206},
}

@article{liu_hierarchical_2021,
	title = {Hierarchical {Adaptive} {Pooling} by {Capturing} {High}-order {Dependency} for {Graph} {Representation} {Learning}},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2021.3133646},
	abstract = {Graph neural networks (GNN) have been proven to be mature enough for handling graph-structured data on node-level graph representation learning tasks. However, the graph pooling technique for learning expressive graph-level representation is critical yet still challenging. Existing pooling methods either struggle to capture the local substructure or fail to effectively utilize high-order dependency, thus diminishing the expression capability. In this paper we propose HAP, a hierarchical graph-level representation learning framework, which is adaptively sensitive to graph structures, i.e., HAP clusters local substructures incorporating with high-order dependencies. HAP utilizes a novel cross-level attention mechanism MOA to naturally focus more on close neighborhood while effectively capture higher-order dependency that may contain crucial information. It also learns a global graph content GCont that extracts the graph pattern properties to make the pre- and post-coarsening graph content maintain stable, thus providing global guidance in graph coarsening. This novel innovation also facilitates generalization across graphs with the same form of features. Extensive experiments on ten datasets show that HAP significantly outperforms twelve popular graph pooling methods on graph classification task with an maximum accuracy improvement of 20.18\%, and exceeds the performance of state-of-the-art graph matching and graph similarity learning algorithms by over 3.42\% and 16\%.},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Liu, Ning and Jian, Songlei and Li, Dongsheng and Zhang, Yiming and Lai, Zhiquan and Xu, Hongzuo},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Neural networks, Convolution, Task analysis, Attention Mechanism, Feature extraction, Graph Pooling, Graph Representation Learning, Hierarchical Manner, Representation learning, Technological innovation, Topology},
	pages = {1--1},
}

@article{osei-kuffuor_matrix_2015,
	title = {Matrix  {Reordering} {Using} {Multilevel}  {Graph} {Coarsening} for {ILU} {Preconditioning}},
	volume = {37},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/abs/10.1137/130936610},
	doi = {10.1137/130936610},
	abstract = {Incomplete LU factorization (ILU) techniques are a well-known class of preconditioners, often used in conjunction with Krylov accelerators for the iterative solution of linear systems of equations. However, for certain problems, ILU factorizations can yield factors that are unstable and in some cases quite dense. Reordering techniques based on permuting the matrix prior to performing the factorization have been shown to improve the quality of the factorization, and the resulting preconditioner. In this paper, we examine the effect of reordering techniques based on multilevel graph coarsening ideas on one-level ILU factorizations, such as the level-based ILU(
k
k
) or the dual threshold ILUT algorithms. We consider an aggregation-based coarsening idea that implements two main coarsening frameworks---a top-down approach, and a bottom-up approach---each utilizing one of two different strategies to select the next-level coarse graph. Numerical results are presented to support our findings.},
	number = {1},
	urldate = {2022-11-28},
	journal = {SIAM Journal on Scientific Computing},
	author = {Osei-Kuffuor, Daniel and Li, Ruipeng and Saad, Yousef},
	month = jan,
	year = {2015},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {65F10, 65F08, 65F50, 65N22, 65Y20, algebraic preconditioners, ILU preconditioners, incomplete factorization preconditioners, multilevel graph coarsening, sparse matrix reordering},
	pages = {A391--A419},
}

@article{ubaru_sampling_2019,
	title = {Sampling and multilevel coarsening algorithms for fast matrix approximations},
	volume = {26},
	issn = {1099-1506},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nla.2234},
	doi = {10.1002/nla.2234},
	abstract = {This paper addresses matrix approximation problems for matrices that are large, sparse, and/or representations of large graphs. To tackle these problems, we consider algorithms that are based primarily on coarsening techniques, possibly combined with random sampling. A multilevel coarsening technique is proposed, which utilizes a hypergraph associated with the data matrix and a graph coarsening strategy based on column matching. We consider a number of standard applications of this technique as well as a few new ones. Among standard applications, we first consider the problem of computing partial singular value decomposition, for which a combination of sampling and coarsening yields significantly improved singular value decomposition results relative to sampling alone. We also consider the column subset selection problem, a popular low-rank approximation method used in data-related applications, and show how multilevel coarsening can be adapted for this problem. Similarly, we consider the problem of graph sparsification and show how coarsening techniques can be employed to solve it. We also establish theoretical results that characterize the approximation error obtained and the quality of the dimension reduction achieved by a coarsening step, when a proper column matching strategy is employed. Numerical experiments illustrate the performances of the methods in a few applications.},
	language = {en},
	number = {3},
	urldate = {2022-11-28},
	journal = {Numerical Linear Algebra with Applications},
	author = {Ubaru, Shashanka and Saad, Yousef},
	year = {2019},
	keywords = {coarsening, multilevel methods, randomization, singular values, subspace iteration, SVD},
	pages = {e2234},
}

@inproceedings{yuan_which_2021,
	address = {New York, NY, USA},
	series = {{GECCO} '21},
	title = {Which hyperparameters to optimise? an investigation of evolutionary hyperparameter optimisation in graph neural network for molecular property prediction},
	isbn = {978-1-4503-8351-6},
	shorttitle = {Which hyperparameters to optimise?},
	url = {https://doi.org/10.1145/3449726.3463192},
	doi = {10.1145/3449726.3463192},
	abstract = {Most GNNs for molecular property prediction are proposed based on the idea of learning the representations for the nodes by aggregating the information of their neighbour nodes in graph layers. Then, the representations can be passed to subsequent task-specific layers to deal with individual downstream tasks. Facing real-world molecular problems, the hyperparameter optimisation for those layers are vital. In this research, we focus on the impact of selecting two types of GNN hyperparameters, those belonging to graph layers and those of task-specific layers, on the performance of GNN for molecular property prediction. In our experiments, we employed a state-of-the-art evolutionary algorithm (i.e., CMA-ES) for HPO. The results reveal that optimising the two types of hyperparameters separately can improve GNNs' performance, but optimising both types of hyperparameters simultaneously will lead to predominant improvements.},
	urldate = {2022-11-28},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} {Companion}},
	publisher = {Association for Computing Machinery},
	author = {Yuan, Yingfang and Wang, Wenjun and Pang, Wei},
	month = jul,
	year = {2021},
	keywords = {graph neural networks, evolutionary computation, hyperparameter optimisation, molecular property prediction},
	pages = {1403--1404},
}

@inproceedings{catalyurek_multithreaded_2012,
	title = {Multithreaded {Clustering} for {Multi}-level {Hypergraph} {Partitioning}},
	doi = {10.1109/IPDPS.2012.81},
	abstract = {Requirements for efficient parallelization of many complex and irregular applications can be cast as a hyper graph partitioning problem. The current-state-of-the art software libraries that provide tool support for the hyper graph partitioning problem are designed and implemented before the game-changing advancements in multi-core computing. Hence, analyzing the structure of those tools for designing multithreaded versions of the algorithms is a crucial tasks. The most successful partitioning tools are based on the multi-level approach. In this approach, a given hyper graph is coarsened to a much smaller one, a partition is obtained on the the smallest hyper graph, and that partition is projected to the original hyper graph while refining it on the intermediate hyper graphs. The coarsening operation corresponds to clustering the vertices of a hyper graph and is the most time consuming task in a multi-level partitioning tool. We present three efficient multithreaded clustering algorithms which are very suited for multi-level partitioners. We compare their performance with that of the ones currently used in today's hyper graph partitioners. We show on a large number of real life hyper graphs that our implementations, integrated into a commonly used partitioning library PaToH, achieve good speedups without reducing the clustering quality.},
	booktitle = {2012 {IEEE} 26th {International} {Parallel} and {Distributed} {Processing} {Symposium}},
	author = {{\c C}ataly{\"u}rek, {\"U}mit V. and Deveci, Mehmet and Kaya, Kamer and U{\c c}ar, Bora},
	month = may,
	year = {2012},
	note = {ISSN: 1530-2075},
	keywords = {coarsening, Algorithm design and analysis, Approximation algorithms, Clustering algorithms, Heuristic algorithms, Measurement, Multi-level hypergraph partitioning, multicore programming, multithreaded clustering algorithms, Partitioning algorithms, Pins},
	pages = {848--859},
}

@inproceedings{zhu_beyond_2020,
	address = {online},
	title = {Beyond {Homophily} in {Graph} {Neural} {Networks}: {Current} {Limitations} and {Effective} {Designs}},
	volume = {33},
	shorttitle = {Beyond {Homophily} in {Graph} {Neural} {Networks}},
	url = {https://proceedings.neurips.cc/paper/2020/file/58ae23d878a47004366189884c2f8440-Paper.pdf},
	abstract = {We investigate the representation power of graph neural networks in the semi-supervised node classification task under heterophily or low homophily, i.e., in networks where connected nodes may have different class labels and dissimilar features. Many popular GNNs fail to generalize to this setting, and are even outperformed by models that ignore the graph structure (e.g., multilayer perceptrons). Motivated by this limitation, we identify a set of key designs{\textemdash}ego- and neighbor-embedding separation, higher-order neighborhoods, and combination of intermediate representations{\textemdash}that boost learning from the graph structure under heterophily. We combine them into a graph neural network, H2GCN, which we use as the base method to empirically evaluate the effectiveness of the identified designs. Going beyond the traditional benchmarks with strong homophily, our empirical analysis shows that the identified designs increase the accuracy of GNNs by up to 40\% and 27\% over models without them on synthetic and real networks with heterophily, respectively, and yield competitive performance under homophily.},
	urldate = {2022-11-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhu, Jiong and Yan, Yujun and Zhao, Lingxiao and Heimann, Mark and Akoglu, Leman and Koutra, Danai},
	year = {2020},
	pages = {7793--7804},
}

@misc{pei_geom-gcn_2020,
	title = {Geom-{GCN}: {Geometric} {Graph} {Convolutional} {Networks}},
	shorttitle = {Geom-{GCN}},
	url = {http://arxiv.org/abs/2002.05287},
	doi = {10.48550/arXiv.2002.05287},
	abstract = {Message-passing neural networks (MPNNs) have been successfully applied to representation learning on graphs in a variety of real-world applications. However, two fundamental weaknesses of MPNNs' aggregators limit their ability to represent graph-structured data: losing the structural information of nodes in neighborhoods and lacking the ability to capture long-range dependencies in disassortative graphs. Few studies have noticed the weaknesses from different perspectives. From the observations on classical neural network and network geometry, we propose a novel geometric aggregation scheme for graph neural networks to overcome the two weaknesses. The behind basic idea is the aggregation on a graph can benefit from a continuous space underlying the graph. The proposed aggregation scheme is permutation-invariant and consists of three modules, node embedding, structural neighborhood, and bi-level aggregation. We also present an implementation of the scheme in graph convolutional networks, termed Geom-GCN (Geometric Graph Convolutional Networks), to perform transductive learning on graphs. Experimental results show the proposed Geom-GCN achieved state-of-the-art performance on a wide range of open datasets of graphs. Code is available at https://github.com/graphdml-uiuc-jlu/geom-gcn.},
	urldate = {2022-11-28},
	publisher = {arXiv},
	author = {Pei, Hongbin and Wei, Bingzhe and Chang, Kevin Chen-Chuan and Lei, Yu and Yang, Bo},
	month = feb,
	year = {2020},
	note = {arXiv:2002.05287 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{lim_large_2021,
	address = {online},
	title = {Large {Scale} {Learning} on {Non}-{Homophilous} {Graphs}: {New} {Benchmarks} and {Strong} {Simple} {Methods}},
	volume = {34},
	shorttitle = {Large {Scale} {Learning} on {Non}-{Homophilous} {Graphs}},
	url = {https://proceedings.neurips.cc/paper/2021/file/ae816a80e4c1c56caa2eb4e1819cbb2f-Paper.pdf},
	abstract = {Many widely used datasets for graph machine learning tasks have generally been homophilous, where nodes with similar labels connect to each other. Recently, new Graph Neural Networks (GNNs) have been developed that move beyond the homophily regime; however, their evaluation has often been conducted on small graphs with limited application domains. We collect and introduce diverse non-homophilous datasets from a variety of application areas that have up to 384x more nodes and 1398x more edges than prior datasets. We further show that existing scalable graph learning and graph minibatching techniques lead to performance degradation on these non-homophilous datasets, thus highlighting the need for further work on scalable non-homophilous methods. To address these concerns, we introduce LINKX --- a strong simple method that admits straightforward minibatch training and inference. Extensive experimental results with representative simple methods and GNNs across our proposed datasets show that LINKX achieves state-of-the-art performance for learning on non-homophilous graphs. Our codes and data are available at https://github.com/CUAI/Non-Homophily-Large-Scale.},
	urldate = {2022-11-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lim, Derek and Hohne, Felix and Li, Xiuyu and Huang, Sijia Linda and Gupta, Vaishnavi and Bhalerao, Omkar and Lim, Ser Nam},
	year = {2021},
	pages = {20887--20902},
}

@article{newman_mixing_2003,
	title = {Mixing patterns in networks},
	volume = {67},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.67.026126},
	doi = {10.1103/PhysRevE.67.026126},
	abstract = {We study assortative mixing in networks, the tendency for vertices in networks to be connected to other vertices that are like (or unlike) them in some way. We consider mixing according to discrete characteristics such as language or race in social networks and scalar characteristics such as age. As a special example of the latter we consider mixing according to vertex degree, i.e., according to the number of connections vertices have to other vertices: do gregarious people tend to associate with other gregarious people? We propose a number of measures of assortative mixing appropriate to the various mixing types, and apply them to a variety of real-world networks, showing that assortative mixing is a pervasive phenomenon found in many networks. We also propose several models of assortatively mixed networks, both analytic ones based on generating function methods, and numerical ones based on Monte Carlo graph generation techniques. We use these models to probe the properties of networks as their level of assortativity is varied. In the particular case of mixing by degree, we find strong variation with assortativity in the connectivity of the network and in the resilience of the network to the removal of vertices.},
	number = {2},
	urldate = {2022-11-28},
	journal = {Physical Review E},
	author = {Newman, M. E. J.},
	month = feb,
	year = {2003},
	note = {Publisher: American Physical Society},
	pages = {026126},
}

@misc{platonov_characterizing_2022,
	title = {Characterizing {Graph} {Datasets} for {Node} {Classification}: {Beyond} {Homophily}-{Heterophily} {Dichotomy}},
	shorttitle = {Characterizing {Graph} {Datasets} for {Node} {Classification}},
	url = {http://arxiv.org/abs/2209.06177},
	doi = {10.48550/arXiv.2209.06177},
	abstract = {Homophily is a graph property describing the tendency of edges to connect similar nodes; the opposite is called heterophily. While homophily is natural for many real-world networks, there are also networks without this property. It is often believed that standard message-passing graph neural networks (GNNs) do not perform well on non-homophilous graphs, and thus such datasets need special attention. While a lot of effort has been put into developing graph representation learning methods for heterophilous graphs, there is no universally agreed upon measure of homophily. Several metrics for measuring homophily have been used in the literature, however, we show that all of them have critical drawbacks preventing comparison of homophily levels between different datasets. We formalize desirable properties for a proper homophily measure and show how existing literature on the properties of classification performance metrics can be linked to our problem. In doing so we find a measure that we call adjusted homophily that satisfies more desirable properties than existing homophily measures. Interestingly, this measure is related to two classification performance metrics - Cohen's Kappa and Matthews correlation coefficient. Then, we go beyond the homophily-heterophily dichotomy and propose a new property that we call label informativeness (LI) that characterizes how much information a neighbor's label provides about a node's label. We theoretically show that LI is comparable across datasets with different numbers of classes and class size balance. Through a series of experiments we show that LI is a better predictor of the performance of GNNs on a dataset than homophily. We show that LI explains why GNNs can sometimes perform well on heterophilous datasets - a phenomenon recently observed in the literature.},
	urldate = {2022-11-28},
	publisher = {arXiv},
	author = {Platonov, Oleg and Kuznedelev, Denis and Babenko, Artem and Prokhorenkova, Liudmila},
	month = sep,
	year = {2022},
	note = {arXiv:2209.06177 [cs, math]},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Computer Science - Discrete Mathematics, Mathematics - Probability},
}

@inproceedings{andersen_local_2006,
	title = {Local {Graph} {Partitioning} using {PageRank} {Vectors}},
	doi = {10.1109/FOCS.2006.44},
	abstract = {A local graph partitioning algorithm finds a cut near a specified starting vertex, with a running time that depends largely on the size of the small side of the cut, rather than the size of the input graph. In this paper, we present a local partitioning algorithm using a variation of PageRank with a specified starting distribution. We derive a mixing result for PageRank vectors similar to that for random walks, and show that the ordering of the vertices produced by a PageRank vector reveals a cut with small conductance. In particular, we show that for any set C with conductance Phi and volume k, a PageRank vector with a certain starting distribution can be used to produce a set with conductance (O(radic(Phi log k)). We present an improved algorithm for computing approximate PageRank vectors, which allows us to find such a set in time proportional to its size. In particular, we can find a cut with conductance at most oslash, whose small side has volume at least 2b in time O(2 log m/(2b log2 m/oslash2) where m is the number of edges in the graph. By combining small sets found by this local partitioning algorithm, we obtain a cut with conductance oslash and approximately optimal balance in time O(m log4 m/oslash)},
	booktitle = {2006 47th {Annual} {IEEE} {Symposium} on {Foundations} of {Computer} {Science} ({FOCS}'06)},
	author = {Andersen, Reid and Chung, Fan and Lang, Kevin},
	month = oct,
	year = {2006},
	note = {ISSN: 0272-5428},
	keywords = {Probability distribution, Algorithm design and analysis, Clustering algorithms, Partitioning algorithms, Analytical models, Computational modeling, Educational institutions, Linear systems, Mathematics, Vectors},
	pages = {475--486},
}

@inproceedings{kloster_heat_2014,
	address = {New York, NY, USA},
	series = {{KDD} '14},
	title = {Heat kernel based community detection},
	isbn = {978-1-4503-2956-9},
	url = {https://doi.org/10.1145/2623330.2623706},
	doi = {10.1145/2623330.2623706},
	abstract = {The heat kernel is a type of graph diffusion that, like the much-used personalized PageRank diffusion, is useful in identifying a community nearby a starting seed node. We present the first deterministic, local algorithm to compute this diffusion and use that algorithm to study the communities that it produces. Our algorithm is formally a relaxation method for solving a linear system to estimate the matrix exponential in a degree-weighted norm. We prove that this algorithm stays localized in a large graph and has a worst-case constant runtime that depends only on the parameters of the diffusion, not the size of the graph. On large graphs, our experiments indicate that the communities produced by this method have better conductance than those produced by PageRank, although they take slightly longer to compute. On a real-world community identification task, the heat kernel communities perform better than those from the PageRank diffusion.},
	urldate = {2022-12-02},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Kloster, Kyle and Gleich, David F.},
	month = aug,
	year = {2014},
	keywords = {heat kernel, local clustering},
	pages = {1386--1395},
}

@article{peel_multiscale_2018,
	title = {Multiscale mixing patterns in networks},
	volume = {115},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1713019115},
	doi = {10.1073/pnas.1713019115},
	abstract = {Assortative mixing in networks is the tendency for nodes with the same attributes, or metadata, to link to each other. It is a property often found in social networks, manifesting as a higher tendency of links occurring between people of the same age, race, or political belief. Quantifying the level of assortativity or disassortativity (the preference of linking to nodes with different attributes) can shed light on the organization of complex networks. It is common practice to measure the level of assortativity according to the assortativity coefficient, or modularity in the case of categorical metadata. This global value is the average level of assortativity across the network and may not be a representative statistic when mixing patterns are heterogeneous. For example, a social network spanning the globe may exhibit local differences in mixing patterns as a consequence of differences in cultural norms. Here, we introduce an approach to localize this global measure so that we can describe the assortativity, across multiple scales, at the node level. Consequently, we are able to capture and qualitatively evaluate the distribution of mixing patterns in the network. We find that, for many real-world networks, the distribution of assortativity is skewed, overdispersed, and multimodal. Our method provides a clearer lens through which we can more closely examine mixing patterns in networks.},
	number = {16},
	urldate = {2022-12-07},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Peel, Leto and Delvenne, Jean-Charles and Lambiotte, Renaud},
	month = apr,
	year = {2018},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {4057--4062},
}

@article{benavoli_time_2017,
	title = {Time for a {Change}: a {Tutorial} for {Comparing} {Multiple} {Classifiers} {Through} {Bayesian} {Analysis}},
	volume = {18},
	issn = {1533-7928},
	shorttitle = {Time for a {Change}},
	url = {http://jmlr.org/papers/v18/16-305.html},
	abstract = {The machine learning community adopted the use of null hypothesis significance testing (NHST) in order to ensure the statistical validity of results. Many scientific fields however realized the shortcomings of frequentist reasoning and in the most radical cases even banned its use in publications. We should do the same: just as we have embraced the Bayesian paradigm in the development of new machine learning methods, so we should also use it in the analysis of our own results. We argue for abandonment of NHST by exposing its fallacies and, more importantly, offer better---more sound and useful--- alternatives for it.},
	number = {77},
	urldate = {2022-12-07},
	journal = {Journal of Machine Learning Research},
	author = {Benavoli, Alessio and Corani, Giorgio and Dem{\v s}ar, Janez and Zaffalon, Marco},
	year = {2017},
	pages = {1--36},
}

@inproceedings{benavoli_bayesian_2014,
	address = {Beijing, China},
	title = {A {Bayesian} {Wilcoxon} signed-rank test based on the {Dirichlet} process},
	url = {https://proceedings.mlr.press/v32/benavoli14.html},
	abstract = {Bayesian methods are ubiquitous in machine learning.  Nevertheless, the analysis of empirical results is typically   performed  by frequentist tests. This implies dealing with  null hypothesis significance tests and  p-values, even though the   shortcomings of such methods are well known.   We propose  a nonparametric Bayesian version of the Wilcoxon   signed-rank test using a Dirichlet process (DP) based prior.  We address in two different ways the problem of how to choose  the   infinite dimensional parameter that characterizes the DP.   The proposed  test has all the traditional strengths of the Bayesian   approach; for instance, unlike the frequentist tests,   it allows verifying the null hypothesis, not only rejecting it, and   taking decision which minimize the expected loss.  Moreover, one of the solutions proposed to model the infinitedimensional parameter of the DP, allows isolating instances in which the traditional frequentist test is guessing at random.   We show results dealing with the comparison of two classifiers using real and simulated data.},
	language = {en},
	urldate = {2022-12-07},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Benavoli, Alessio and Corani, Giorgio and Mangili, Francesca and Zaffalon, Marco and Ruggeri, Fabrizio},
	month = jun,
	year = {2014},
	note = {ISSN: 1938-7228},
	pages = {1026--1034},
}

@inproceedings{fu_magnn_2020,
	title = {{MAGNN}: {Metapath} {Aggregated} {Graph} {Neural} {Network} for {Heterogeneous} {Graph} {Embedding}},
	shorttitle = {{MAGNN}},
	url = {http://arxiv.org/abs/2002.01680},
	doi = {10.1145/3366423.3380297},
	abstract = {A large number of real-world graphs or networks are inherently heterogeneous, involving a diversity of node types and relation types. Heterogeneous graph embedding is to embed rich structural and semantic information of a heterogeneous graph into low-dimensional node representations. Existing models usually define multiple metapaths in a heterogeneous graph to capture the composite relations and guide neighbor selection. However, these models either omit node content features, discard intermediate nodes along the metapath, or only consider one metapath. To address these three limitations, we propose a new model named Metapath Aggregated Graph Neural Network (MAGNN) to boost the final performance. Specifically, MAGNN employs three major components, i.e., the node content transformation to encapsulate input node attributes, the intra-metapath aggregation to incorporate intermediate semantic nodes, and the inter-metapath aggregation to combine messages from multiple metapaths. Extensive experiments on three real-world heterogeneous graph datasets for node classification, node clustering, and link prediction show that MAGNN achieves more accurate prediction results than state-of-the-art baselines.},
	urldate = {2023-04-03},
	booktitle = {Proceedings of {The} {Web} {Conference} 2020},
	author = {Fu, Xinyu and Zhang, Jiani and Meng, Ziqiao and King, Irwin},
	month = apr,
	year = {2020},
	note = {arXiv:2002.01680 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks},
	pages = {2331--2341},
}

@misc{shchur_pitfalls_2019,
	title = {Pitfalls of {Graph} {Neural} {Network} {Evaluation}},
	url = {http://arxiv.org/abs/1811.05868},
	doi = {10.48550/arXiv.1811.05868},
	abstract = {Semi-supervised node classification in graphs is a fundamental problem in graph mining, and the recently proposed graph neural networks (GNNs) have achieved unparalleled results on this task. Due to their massive success, GNNs have attracted a lot of attention, and many novel architectures have been put forward. In this paper we show that existing evaluation strategies for GNN models have serious shortcomings. We show that using the same train/validation/test splits of the same datasets, as well as making significant changes to the training procedure (e.g. early stopping criteria) precludes a fair comparison of different architectures. We perform a thorough empirical evaluation of four prominent GNN models and show that considering different splits of the data leads to dramatically different rankings of models. Even more importantly, our findings suggest that simpler GNN architectures are able to outperform the more sophisticated ones if the hyperparameters and the training procedure are tuned fairly for all models.},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Shchur, Oleksandr and Mumme, Maximilian and Bojchevski, Aleksandar and G{\"u}nnemann, Stephan},
	month = jun,
	year = {2019},
	note = {arXiv:1811.05868 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
}

@misc{hu_open_2021,
	title = {Open {Graph} {Benchmark}: {Datasets} for {Machine} {Learning} on {Graphs}},
	shorttitle = {Open {Graph} {Benchmark}},
	url = {http://arxiv.org/abs/2005.00687},
	doi = {10.48550/arXiv.2005.00687},
	abstract = {We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale, encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at https://ogb.stanford.edu .},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
	month = feb,
	year = {2021},
	note = {arXiv:2005.00687 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
}

@inproceedings{jin_condensing_2022,
	address = {New York, NY, USA},
	series = {{KDD} '22},
	title = {Condensing {Graphs} via {One}-{Step} {Gradient} {Matching}},
	isbn = {978-1-4503-9385-0},
	url = {https://doi.org/10.1145/3534678.3539429},
	doi = {10.1145/3534678.3539429},
	abstract = {As training deep learning models on large dataset takes a lot of time and resources, it is desired to construct a small synthetic dataset with which we can train deep learning models sufficiently. There are recent works that have explored solutions on condensing image datasets through complex bi-level optimization. For instance, dataset condensation (DC) matches network gradients w.r.t. large-real data and small-synthetic data, where the network weights are optimized for multiple steps at each outer iteration. However, existing approaches have their inherent limitations: (1) they are not directly applicable to graphs where the data is discrete; and (2) the condensation process is computationally expensive due to the involved nested optimization. To bridge the gap, we investigate efficient dataset condensation tailored for graph datasets where we model the discrete graph structure as a probabilistic model. We further propose a one-step gradient matching scheme, which performs gradient matching for only one single step without training the network weights. Our theoretical analysis shows this strategy can generate synthetic graphs that lead to lower classification loss on real graphs. Extensive experiments on various graph datasets demonstrate the effectiveness and efficiency of the proposed method. In particular, we are able to reduce the dataset size by 90\% while approximating up to 98\% of the original performance and our method is significantly faster than multi-step gradient matching (e.g. \$15\${\texttimes} in CIFAR10 for synthesizing 500 graphs).},
	urldate = {2023-06-16},
	booktitle = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Jin, Wei and Tang, Xianfeng and Jiang, Haoming and Li, Zheng and Zhang, Danqing and Tang, Jiliang and Yin, Bing},
	month = aug,
	year = {2022},
	keywords = {data-efficient learning, graph generation, graph neural networks},
	pages = {720--730},
}

@inproceedings{jin_graph_2022,
	title = {Graph {Condensation} for {Graph} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=WLEx3Jo4QaB},
	abstract = {Given the prevalence of large-scale graphs in real-world applications, the storage and time for training neural models have raised increasing concerns. To alleviate the concerns, we propose and study the problem of graph condensation for graph neural networks (GNNs). Specifically, we aim to condense the large, original graph into a small, synthetic and highly-informative graph, such that GNNs trained on the small graph and large graph have comparable performance. We approach the condensation problem by imitating the GNN training trajectory on the original graph through the optimization of a gradient matching loss and design a strategy to condense node futures and structural information simultaneously. Extensive experiments have demonstrated the effectiveness of the proposed framework in condensing different graph datasets into informative smaller graphs. In particular, we are able to approximate the original test accuracy by 95.3{\textbackslash}\% on Reddit, 99.8{\textbackslash}\% on Flickr and 99.0{\textbackslash}\% on Citeseer, while reducing their graph size by more than 99.9{\textbackslash}\%, and the condensed graphs can be used to train various GNN architectures.},
	language = {en},
	urldate = {2023-06-16},
	booktitle = {The {Tenth} {International} {Conference} on {Learning} {Representations}},
	author = {Jin, Wei and Zhao, Lingxiao and Zhang, Shichang and Liu, Yozen and Tang, Jiliang and Shah, Neil},
	month = jan,
	year = {2022},
}

@article{grattarola_understanding_2022,
	title = {Understanding {Pooling} in {Graph} {Neural} {Networks}},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2022.3190922},
	abstract = {Many recent works in the field of graph machine learning have introduced pooling operators to reduce the size of graphs. In this article, we present an operational framework to unify this vast and diverse literature by describing pooling operators as the combination of three functions: selection, reduction, and connection (SRC). We then introduce a taxonomy of pooling operators, based on some of their key characteristics and implementation differences under the SRC framework. Finally, we propose three criteria to evaluate the performance of pooling operators and use them to investigate the behavior of different operators on a variety of tasks.},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Grattarola, Daniele and Zambon, Daniele and Bianchi, Filippo Maria and Alippi, Cesare},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Aggregates, Clustering algorithms, Convolution, Dimensionality reduction, graph neural networks (GNNs), Laplace equations, Point cloud compression, Task analysis, Taxonomy},
	pages = {1--11},
}

@inproceedings{bacciu_non-negative_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Non}-negative {Factorization} {Approach} to {Node} {Pooling} in {Graph} {Convolutional} {Neural} {Networks}},
	isbn = {978-3-030-35166-3},
	doi = {10.1007/978-3-030-35166-3_21},
	abstract = {The paper discusses a pooling mechanism to induce subsampling in graph structured data and introduces it as a component of a graph convolutional neural network. The pooling mechanism builds on the Non-Negative Matrix Factorization (NMF) of a matrix representing node adjacency and node similarity as adaptively obtained through the vertices embedding learned by the model. Such mechanism is applied to obtain an incrementally coarser graph where nodes are adaptively pooled into communities based on the outcomes of the non-negative factorization. The empirical analysis on graph classification benchmarks shows how such coarsening process yields significant improvements in the predictive performance of the model with respect to its non-pooled counterpart.},
	language = {en},
	booktitle = {{AI}*{IA} 2019 {\textendash} {Advances} in {Artificial} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Bacciu, Davide and Di Sotto, Luigi},
	editor = {Alviano, Mario and Greco, Gianluigi and Scarcello, Francesco},
	year = {2019},
	keywords = {Differentiable graph pooling, Graph Convolutional Neural Networks, Non-Negative Matrix Factorization},
	pages = {294--306},
}

@misc{cangea_towards_2018,
	title = {Towards {Sparse} {Hierarchical} {Graph} {Classifiers}},
	url = {http://arxiv.org/abs/1811.01287},
	doi = {10.48550/arXiv.1811.01287},
	abstract = {Recent advances in representation learning on graphs, mainly leveraging graph convolutional networks, have brought a substantial improvement on many graph-based benchmark tasks. While novel approaches to learning node embeddings are highly suitable for node classification and link prediction, their application to graph classification (predicting a single label for the entire graph) remains mostly rudimentary, typically using a single global pooling step to aggregate node features or a hand-designed, fixed heuristic for hierarchical coarsening of the graph structure. An important step towards ameliorating this is differentiable graph coarsening---the ability to reduce the size of the graph in an adaptive, data-dependent manner within a graph neural network pipeline, analogous to image downsampling within CNNs. However, the previous prominent approach to pooling has quadratic memory requirements during training and is therefore not scalable to large graphs. Here we combine several recent advances in graph neural network design to demonstrate that competitive hierarchical graph classification results are possible without sacrificing sparsity. Our results are verified on several established graph classification benchmarks, and highlight an important direction for future research in graph-based neural networks.},
	urldate = {2023-06-16},
	publisher = {arXiv},
	author = {Cangea, C{\u a}t{\u a}lina and Veli{\v c}kovi{\'c}, Petar and Jovanovi{\'c}, Nikola and Kipf, Thomas and Li{\`o}, Pietro},
	month = nov,
	year = {2018},
	note = {arXiv:1811.01287 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
}

@inproceedings{gao_graph_2019,
	title = {Graph {U}-{Nets}},
	url = {https://proceedings.mlr.press/v97/gao19a.html},
	abstract = {We consider the problem of representation learning for graph data. Convolutional neural networks can naturally operate on images, but have significant challenges in dealing with graph data. Given images are special cases of graphs with nodes lie on 2D lattices, graph embedding tasks have a natural correspondence with image pixel-wise prediction tasks such as segmentation. While encoder-decoder architectures like U-Nets have been successfully applied on many image pixel-wise prediction tasks, similar methods are lacking for graph data. This is due to the fact that pooling and up-sampling operations are not natural on graph data. To address these challenges, we propose novel graph pooling (gPool) and unpooling (gUnpool) operations in this work. The gPool layer adaptively selects some nodes to form a smaller graph based on their scalar projection values on a trainable projection vector. We further propose the gUnpool layer as the inverse operation of the gPool layer. The gUnpool layer restores the graph into its original structure using the position information of nodes selected in the corresponding gPool layer. Based on our proposed gPool and gUnpool layers, we develop an encoder-decoder model on graph, known as the graph U-Nets. Our experimental results on node classification and graph classification tasks demonstrate that our methods achieve consistently better performance than previous models.},
	language = {en},
	urldate = {2023-06-16},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Gao, Hongyang and Ji, Shuiwang},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2083--2092},
}

@inproceedings{zhao_dataset_2021,
	title = {Dataset {Condensation} with {Gradient} {Matching}},
	url = {https://openreview.net/forum?id=mSAKhLYLSsl},
	abstract = {As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available.},
	language = {en},
	urldate = {2023-06-16},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhao, Bo and Mopuri, Konda Reddy and Bilen, Hakan},
	month = jan,
	year = {2021},
}
